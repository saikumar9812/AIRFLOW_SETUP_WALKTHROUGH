# End-to-End ETL Project Setup & Deployment — Documentation

This document outlines all the steps I followed — from local development in VS Code to deploying a fully automated ETL pipeline on Astronomer Cloud integrated with GitHub. It is meant to guide someone new through the complete process, without disclosing any project-specific details or sensitive configurations.

---

## 1. Initial Project Setup in VS Code

### Folder Structure (Sample)

```
project_folder/
├── dags/
│   ├── [all Python ETL logic files go here]
├── requirements.txt
├── Dockerfile
├── .astro/
│   └── config.yaml
```

### Key Files

- Modular ETL scripts written in Python
- Airflow DAGs to define orchestration logic
- Dependency files for building Docker image
- Runtime config for Astronomer deployment

---

## 2. Local Airflow Environment with Astronomer CLI

### Step 1: Install Requirements

- WSL (Ubuntu)
- Docker Desktop (with WSL integration enabled)
- Astro CLI (installed using the following command):

```bash
curl -sSL https://install.astronomer.io | sudo bash
```

### Step 2: Initialize Airflow Project

```bash
mkdir -p ~/airflow_project/dags
cd ~/airflow_project
astro dev init
```

This generates the necessary folder structure including `.astro/config.yaml`, `Dockerfile`, and other configuration scaffolds.

### Step 3: Copy Your Project Files

Move your local ETL project files into the `~/airflow_project/dags/` directory:

```bash
cp /mnt/c/Users/YourName/Downloads/your_project_folder/* ~/airflow_project/dags/
```

### Step 4: Add Runtime Dependencies

Edit or create a `requirements.txt` in the root folder:

```
snowflake-connector-python
pandas
openpyxl
odfpy
pycryptodome
requests
```

### Step 5: Update Dockerfile to Install Dependencies

```dockerfile
FROM quay.io/astronomer/astro-runtime:12.8.0
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
```

This ensures that your Airflow Docker image contains all the packages needed by your scripts.

### Step 6: Free Up Ports (Optional)

```bash
sudo service postgresql stop
```

(Port 5432 may conflict with local Postgres; Astronomer uses it inside containers)

### Step 7: Build and Start Local Airflow

```bash
astro dev restart
```

This will:

- Build a new Docker image based on your `Dockerfile` and `requirements.txt`
- Start Airflow containers (webserver, scheduler, Postgres, triggerer)
- Expose UI on [http://localhost:8080](http://localhost:8080) (admin/admin)

---

## 3. Deployment to Astronomer Cloud

### Step 1: Create Astronomer Cloud Account

Go to [app.astronomer.io](https://app.astronomer.io)

- Create a workspace
- Launch a new deployment (choose latest Airflow runtime)

### Step 2: Authenticate to Astronomer via CLI

```bash
astro login
```

Follow the browser-based login process. This links CLI to your Astro account.

### Step 3: Deploy to Cloud

```bash
astro deploy
```

You will be prompted to select the deployment. After successful image build and push, you'll get:

- Hosted Airflow UI link
- Deployment status in the Astronomer dashboard

Note: Astronomer builds a container image from your project and pushes it to its registry automatically.

---

## 4. Optional GitHub-Based CI/CD Integration

### GitHub Repo Minimum Structure:

```
my-etl-repo/
├── dags/
│   └── *.py
├── requirements.txt
├── Dockerfile
├── .astro/config.yaml
```

### Setup Git Sync in Astronomer Cloud

- Navigate to Deployment > Settings > Git Sync
- Connect GitHub account and choose your repository/branch
- Enable auto-deploy on push

Once set:

- Every commit triggers a rebuild
- Activity logs show the build status
- DAGs auto-refresh inside Astronomer UI

---

## 5. Common Errors & Resolutions

| Error Message                    | Resolution                                                        |
| -------------------------------- | ----------------------------------------------------------------- |
| `port 5432 already in use`       | Stop system PostgreSQL or change Astronomer port to 5433          |
| `ModuleNotFoundError`            | Add the missing package to `requirements.txt` and restart         |
| `PermissionError writing files`  | Avoid writing to `/usr/local/airflow/dags`, use `/tmp/` instead   |
| DAG not showing                  | Make sure files are placed in `dags/` and contain no errors       |
| GitHub sync not triggering build | Ensure `.astro/config.yaml`, `Dockerfile`, and DAGs exist in root |

---

## 6. Final Remarks

This end-to-end process covered:

- Containerized ETL development
- Secure credential encryption
- Local Airflow DAG testing
- Production deployment to Astronomer Cloud
- GitHub-driven auto-deployment setup

By bridging local and cloud-native development, this approach ensures scalability, repeatability, and security of modern ETL workflows.

